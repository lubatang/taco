\section{Introduction}
% general statements of the area
The advance of heterogeneous computing drives on-device compilation and linking. GPUs have transitioned from pure graphics accelerators to general-purpose parallel processors, supported by standard APIs and tools, such as \cite{CUDA}, \cite{OpenCL}, \cite{DirectCompute} and \cite{HSA}. In order to reduce build time and to create 3rd party libraries, \cite{CUDA}, \cite{OpenCL} and \cite{HSA} coincidentally start to separate compilation and linking of programs. Program sources can be compiled to generate a binary object and linked in a separate stage with other compiled objects. In the new standards, not only the compilation and but also the linking take place on devices.

% identify the research topic within the area
Limited optimizations of on-device compilers make linker optimizations more noticeable. In the mobile device world, computing power and memory size are limited; hence on-device compilers usually don't employ complex optimizations. On-device linkers have the whole-program overview to apply fast optimizations. Therefore, they have the capacity as superior optimizers which are actually complementary to compiler optimizations. Unfortunately, typical linkers do not have clear intermediate representation (IR) for optimization purpose. The lack of IR limits the development of the area of linker optimization. As the results, most of these whole-program optimizations are in fact local. That is, they transform only small portion of a program at a time. Research of linking speed is another depressed area. Traditional linkers recklessly read the input files again and again. Unnecessary file I/O operations apparently slow down the linking process. Moreover, most linkers use some complicated bookkeeping to track symbol and relocation information. Unnecessary, intricate bookkeeping causes these linkers becomes nothing more than a speed bump.

% contributions
In this paper, we propose MCLinker, a portable, retargetable linker framework to support on-device linking. MCLinker defines a novel, mathematic-defined IR to archive high performance and low memory usage simultaneously. MCLinker IR is two-level: one is a syntax tree of the command line language, and the other level is fragment-reference graph used to describe all use-define relationships of the whole program. The first-level syntax tree saves unnecessary bookkeepings and prevents MCLinker from over read the input files. The second-level IR enables more aggressive analysis and optimization that traditional compilers cannot employ, such as branch-island elimination and instruction relaxing. With the clear MCLinker IR, we show it is possible to separate entire linking process into several distinguishing phases: normalization, resolution, layout and emission. Every phase transforms MCLinker IR from one mathematic definition to the others. We also investigate the fastest algorithm for each phase in this paper. Moreover, we list possible optimizations and point out the appropriate phase for each optimization.

% paper structure
The rest of this paper is organized as follows. Section 2 gives an overview of related work in the area of optimizing linkers. Section 3 describes the MCLinker module representation, the basic concepts and mathematic definitions of MCLinker intermediate representation. Section 4 then describes the design of the MCLinker linker framework. Section 5 discusses our evaluation of the MCLinker framework. We in-depth compare MCLinker with related previous works in Section 6 and draw conclusions in Section 7.
